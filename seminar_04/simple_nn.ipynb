{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Functions \n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>\n",
    "<img src=\"img/bp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"./img/loss.png\" width=\"300\">\n",
    "<img src=\"./img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    L_i = np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "#     print('L_i', L_i)\n",
    "    loss = np.mean(-np.log([L_i[i, y[i]] for i in range(len(y))]))\n",
    "#     print('loss',loss)\n",
    "    \"\"\"∂Li/∂fj= Li_j−1 (if j = y_i)\n",
    "                L_i_j  (if j != y_i)\n",
    "    ∂L/∂fj = 1/N * ∂Li/∂fj           \n",
    "    \"\"\"    \n",
    "    dx = L_i\n",
    "    dx[range(len(y)),y] -= 1\n",
    "    dx /= len(y)\n",
    "    \n",
    "#     loss  = lambda x: rel_error(np.mean(np.exp(x) / np.sum(np.exp(x)), axis=1), y)\n",
    "#     dx = eval_numerical_gradient_array(loss, x, 1)\n",
    "#     print(dx)\n",
    "#     return loss, dx\n",
    "#     return loss\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "# print(np.shape(dx))\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      " 1.12523668114\n"
     ]
    }
   ],
   "source": [
    "print('loss is a scalar\\n', loss(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      " [[ 0.02975702 -0.05510638  0.02534936]\n",
      " [ 0.04707922  0.0283709  -0.07545012]\n",
      " [ 0.03308504  0.03891291 -0.07199795]\n",
      " [ 0.041414   -0.07245492  0.03104093]\n",
      " [ 0.02836072  0.03694576 -0.06530648]\n",
      " [ 0.036536   -0.08031961  0.04378361]\n",
      " [ 0.02630774  0.02663515 -0.05294289]\n",
      " [ 0.03254231 -0.06403329  0.03149098]\n",
      " [-0.07095069  0.03946409  0.03148659]\n",
      " [ 0.02138458  0.04282005 -0.06420463]]\n"
     ]
    }
   ],
   "source": [
    "print('gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3))).reshape((10,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 3.63268948258e-08\n"
     ]
    }
   ],
   "source": [
    "print('difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows. \n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    out = np.matmul(x, w) + b\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "# print('x', x)\n",
    "# print('w', w)\n",
    "# print('b', b)\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "#     dx, dw, db = None, None, None\n",
    "    dx = np.dot(dout, w.T)\n",
    "    dw = np.dot(x.T, dout) \n",
    "    db = np.sum(dout, axis=0)\n",
    "#     print('db', db)\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.8725214002e-10\n",
      "dw error:  2.49366615636e-11\n",
      "db error:  1.05830935702e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "# print(affine_forward(x, w, b)[0])\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x.reshape((x.shape[0], -1)), dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "#     out = None\n",
    "    out = np.maximum(x, 0)\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.04545455  0.13636364]\n",
      " [ 0.22727273  0.31818182  0.40909091  0.5       ]]\n",
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "print(out)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    dx = dout* (x > 0)\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756180686e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[6 6 2 7 9 5 3 9 0 1 8 1 9 6 7 9 4 8 7 2 0 3 6 7 2 0 8 0 9 7 7 3 6 5 9 9 6\n",
      " 9 2 2 3 5 7 2 2 8 8 6 9 3 8 3 6 3 2 3 4 7 3 0 2 6 9 0 0 8 8 7 6 5 5 9 1 1\n",
      " 8 0 3 9 3 6 8 4 8 6 0 1 1 3 5 1 0 4 3 4 2 7 6 0 7 8 2 1 7 9 8 3 4 9 9 5 4\n",
      " 2 7 2 3 3 7 0 9 2 3 7 7 5 4 9 5 9 9 2 7 4 3 7 9 2 4 0 2 4 9 1 7 4 0 1 5 4\n",
      " 0 6 3 2 7 3 7 6 2 5 2 8 4 9 7 9 3 1 9 2 8 5 6 1 0 4 8 1 9 6 1 6 5 7 7 9 4\n",
      " 4 6 5 5 8 7 0 0 3 3 1 3 0 2 4 7 2 3 4 3 0 1 8 5 2 4 8 3 4 7 8 3 4 5 6 1 5\n",
      " 8 7 2 8 6 5 4 1 5 8 1 3 8 7 5 2 3 1 0 2 2 6 0 2 2 3 5 3 7 1 8 2 6 1 4 3 8\n",
      " 3 5 6 2 0 6 1 2 8 7 5 8 8 7 8 3 8 5 6 5 3 4 2 0 4 4 9 1 1 3 3 9 6 9 8 6 8\n",
      " 6 6 1 8 9 0 3 3 4 6 4 5 0 9 8 1 1 4 2 0 1 3 2 8 8 9 3 5 6 0 9 2 5 6 3 9 4\n",
      " 7 0 5 2 6 5 9 3 0 1 1 6 8 0 2 9 4 2 2 0 0 5 2 1 4 1 5 1 2 1 3 7 3 9 8 2 8\n",
      " 4 9 4 7 5 5 9 4 7 5 2 0 9 7 6 8 6 3 3 1 7 5 2 6 7 8 7 3 5 0 8 0 4 0 1 2 0\n",
      " 7 6 9 5 8 1 2 5 7 6 9 0 6 3 1 2 0 3 5 3 3 5 6 2 2 3 0 8 9 0 8 8 9 2 5 9 5\n",
      " 4 3 9 1 8 9 0 0 4 3 9 5 1 9 8 5 0 3 4 0 1 8 5 8 0 4 3 1 9 3 2 5 8 3 9 9 0\n",
      " 7 0 3 0 7 3 0 1 6 0 1 0 8 6 7 6 8 7 8 3 6 1 4 6 1 5 4 3 4 8 5 6 5 8 8 8 9\n",
      " 4 9 2 6 1 7 4 5 1 0 8 7 3 9 2 9 0 5 4 8 7 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "print(y_test)\n",
    "\n",
    "data_max = np.max(X_train)\n",
    "# print('Xtest1', X_test)\n",
    "X_train = X_train/data_max \n",
    "X_test = X_test/data_max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f38a1ac84a8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNG4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f38ab2d5160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "\t tr_loss 13.47\n",
      "\t te_loss 12.06\n",
      "\t te_acc 0.0648148148148\n",
      "epoch 1000:\n",
      "\t tr_loss 2.12\n",
      "\t te_loss 2.11\n",
      "\t te_acc 0.468518518519\n",
      "epoch 2000:\n",
      "\t tr_loss 1.00\n",
      "\t te_loss 1.10\n",
      "\t te_acc 0.703703703704\n",
      "epoch 3000:\n",
      "\t tr_loss 0.71\n",
      "\t te_loss 0.78\n",
      "\t te_acc 0.790740740741\n",
      "epoch 4000:\n",
      "\t tr_loss 0.48\n",
      "\t te_loss 0.61\n",
      "\t te_acc 0.827777777778\n",
      "epoch 5000:\n",
      "\t tr_loss 0.79\n",
      "\t te_loss 0.51\n",
      "\t te_acc 0.851851851852\n",
      "epoch 6000:\n",
      "\t tr_loss 0.26\n",
      "\t te_loss 0.44\n",
      "\t te_acc 0.874074074074\n",
      "epoch 7000:\n",
      "\t tr_loss 0.46\n",
      "\t te_loss 0.39\n",
      "\t te_acc 0.885185185185\n",
      "epoch 8000:\n",
      "\t tr_loss 0.24\n",
      "\t te_loss 0.36\n",
      "\t te_acc 0.896296296296\n",
      "epoch 9000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.33\n",
      "\t te_acc 0.901851851852\n",
      "epoch 10000:\n",
      "\t tr_loss 0.30\n",
      "\t te_loss 0.31\n",
      "\t te_acc 0.911111111111\n",
      "epoch 11000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.29\n",
      "\t te_acc 0.912962962963\n",
      "epoch 12000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.27\n",
      "\t te_acc 0.916666666667\n",
      "epoch 13000:\n",
      "\t tr_loss 0.20\n",
      "\t te_loss 0.26\n",
      "\t te_acc 0.922222222222\n",
      "epoch 14000:\n",
      "\t tr_loss 0.11\n",
      "\t te_loss 0.25\n",
      "\t te_acc 0.925925925926\n",
      "epoch 15000:\n",
      "\t tr_loss 0.27\n",
      "\t te_loss 0.24\n",
      "\t te_acc 0.927777777778\n",
      "epoch 16000:\n",
      "\t tr_loss 0.14\n",
      "\t te_loss 0.23\n",
      "\t te_acc 0.92962962963\n",
      "epoch 17000:\n",
      "\t tr_loss 0.19\n",
      "\t te_loss 0.22\n",
      "\t te_acc 0.931481481481\n",
      "epoch 18000:\n",
      "\t tr_loss 0.08\n",
      "\t te_loss 0.21\n",
      "\t te_acc 0.935185185185\n",
      "epoch 19000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.21\n",
      "\t te_acc 0.937037037037\n",
      "epoch 20000:\n",
      "\t tr_loss 0.18\n",
      "\t te_loss 0.20\n",
      "\t te_acc 0.940740740741\n",
      "epoch 21000:\n",
      "\t tr_loss 0.12\n",
      "\t te_loss 0.19\n",
      "\t te_acc 0.944444444444\n",
      "epoch 22000:\n",
      "\t tr_loss 0.12\n",
      "\t te_loss 0.19\n",
      "\t te_acc 0.946296296296\n",
      "epoch 23000:\n",
      "\t tr_loss 0.06\n",
      "\t te_loss 0.18\n",
      "\t te_acc 0.948148148148\n",
      "epoch 24000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.18\n",
      "\t te_acc 0.948148148148\n",
      "epoch 25000:\n",
      "\t tr_loss 0.06\n",
      "\t te_loss 0.17\n",
      "\t te_acc 0.953703703704\n",
      "epoch 26000:\n",
      "\t tr_loss 0.19\n",
      "\t te_loss 0.17\n",
      "\t te_acc 0.955555555556\n",
      "epoch 27000:\n",
      "\t tr_loss 0.15\n",
      "\t te_loss 0.17\n",
      "\t te_acc 0.957407407407\n",
      "epoch 28000:\n",
      "\t tr_loss 0.10\n",
      "\t te_loss 0.16\n",
      "\t te_acc 0.957407407407\n",
      "epoch 29000:\n",
      "\t tr_loss 0.10\n",
      "\t te_loss 0.16\n",
      "\t te_acc 0.957407407407\n",
      "epoch 30000:\n",
      "\t tr_loss 0.08\n",
      "\t te_loss 0.16\n",
      "\t te_acc 0.957407407407\n",
      "epoch 31000:\n",
      "\t tr_loss 0.08\n",
      "\t te_loss 0.15\n",
      "\t te_acc 0.957407407407\n",
      "epoch 32000:\n",
      "\t tr_loss 0.13\n",
      "\t te_loss 0.15\n",
      "\t te_acc 0.957407407407\n",
      "epoch 33000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.15\n",
      "\t te_acc 0.959259259259\n",
      "epoch 34000:\n",
      "\t tr_loss 0.08\n",
      "\t te_loss 0.15\n",
      "\t te_acc 0.959259259259\n",
      "epoch 35000:\n",
      "\t tr_loss 0.06\n",
      "\t te_loss 0.15\n",
      "\t te_acc 0.959259259259\n",
      "epoch 36000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.14\n",
      "\t te_acc 0.959259259259\n",
      "epoch 37000:\n",
      "\t tr_loss 0.17\n",
      "\t te_loss 0.14\n",
      "\t te_acc 0.961111111111\n",
      "epoch 38000:\n",
      "\t tr_loss 0.05\n",
      "\t te_loss 0.14\n",
      "\t te_acc 0.962962962963\n",
      "epoch 39000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.14\n",
      "\t te_acc 0.961111111111\n",
      "epoch 40000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.14\n",
      "\t te_acc 0.961111111111\n",
      "epoch 41000:\n",
      "\t tr_loss 0.07\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.962962962963\n",
      "epoch 42000:\n",
      "\t tr_loss 0.06\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.964814814815\n",
      "epoch 43000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.964814814815\n",
      "epoch 44000:\n",
      "\t tr_loss 0.06\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.964814814815\n",
      "epoch 45000:\n",
      "\t tr_loss 0.05\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.962962962963\n",
      "epoch 46000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.964814814815\n",
      "epoch 47000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.964814814815\n",
      "epoch 48000:\n",
      "\t tr_loss 0.07\n",
      "\t te_loss 0.13\n",
      "\t te_acc 0.962962962963\n",
      "epoch 49000:\n",
      "\t tr_loss 0.05\n",
      "\t te_loss 0.12\n",
      "\t te_acc 0.966666666667\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.uniform(-1, 1, (64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.uniform(-1,1,(100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-3\n",
    "# print('Xtest2', X_test)\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "#     print('w1', W1)\n",
    "#     print('batch_X', batch_X)\n",
    "#     print('sum 1 row X', np.sum(batch_X[1]))\n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "#     print('out1', out1)\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "#     print('out2', out2)\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer\n",
    "#     print('out3', out3)\n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "#     print('tr_loss', tr_loss)\n",
    "#     print('dx', dx)\n",
    "    # Backward Pass\n",
    "    dx2, dw2, db2 = affine_backward(dx, cache3)\n",
    "#     print('dx2', dx2)\n",
    "    dx12 = relu_backward(dx2, cache2)\n",
    "#     print('dx2.shape', dx2.shape, dx2)\n",
    "#     dx12 = relu_backward(out2, cache2)\n",
    "#     print('dx12.shape',dx12.shape)\n",
    "#     print('dx12', dx12)\n",
    "    dx1, dw1, db1 = affine_backward(dx12, cache1)\n",
    "#     dx1, dw1, db1 = affine_backward(out1, cache1)\n",
    "#     print('dw1.shape', dw1.shape)\n",
    "#     print('dw1', dw1)\n",
    "    # Updates\n",
    "    W2 -= lr*dw2\n",
    "    b2 -= lr*db2\n",
    "    W1 -= lr*dw1\n",
    "#     print('W1', W1)\n",
    "    b1 -= lr*db1\n",
    "#     print('b1', b1)\n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "#     print('Out1', out1)\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "#     print('out3', out3)\n",
    "    te_loss, dx = softmax_loss(out3, y_test)      # Loss Layer\n",
    "#     te_loss = softmax_loss(out3, y_test)[0]\n",
    "    # Predict\n",
    "#     y_pred = ...\n",
    "    y_pred = np.argmax(out3, axis=1)\n",
    "#     print(y_pred.shape)\n",
    "    if i % 1000 == 0:\n",
    "        print('epoch %s:' % i) \n",
    "        print('\\t tr_loss %.2f' % tr_loss)\n",
    "        print('\\t te_loss %.2f' % te_loss)\n",
    "        print('\\t te_acc %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
